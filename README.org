#+TITLE: Language and Voice Laboratory Computing Resources

* Table of Contents                                               :TOC:QUOTE:
#+BEGIN_QUOTE
- [[#introduction][Introduction]]
- [[#scheduler][Scheduler]]
- [[#storage][Storage]]
- [[#useful-places][Useful places]]
- [[#containers][Containers]]
- [[#jupyter-notebooks-jupyterhub][Jupyter Notebooks (JupyterHub)]]
- [[#installing-software][Installing software]]
#+END_QUOTE

* Introduction
  The Language and Voice Laboratory (LVL) runs a tiny computing "cluster" called
  Terra.  This cluster consists of two physical nodes, =terra= and =torpaq=.

  Access is granted by request by a sysadmin in the LVL.  Once you have a user
  account you can log into the main node:

  #+begin_src shell
  ssh user@terra.hir.is
  #+end_src

  Any questions additional questions can be asked on the =Compute= channel on [[https://teams.microsoft.com/l/channel/19%3a4fd5a4ebe61049838ecbb7c30e9c60b6%40thread.tacv2/Compute?groupId=48a01a42-5054-47d1-8f80-e29fa31e2987&tenantId=94d92b60-e6fa-43c0-8005-e73e02812c34][Teams]].
  
  A short slideshow with examples and explinations is available [[https://docs.google.com/presentation/d/19eCrIxmAmAxD3t8mzboKjNVXm6O8tWP_gSNoS2xeXLk/edit?usp=sharing][here]].

* Scheduler
  The LVL cluster uses [[https://slurm.schedmd.org][Slurm]] to handle compute job scheduling and resource
  allocation.  All resource intensive tasks *must* use the scheduling system,
  and please refrain from requesting way more resources than is necessary.

  The command =sbatch= is used to submit batch jobs to the scheduler. This is
  the most common way to run tasks on the cluster. A batch job is described by a
  batch script and the command-line arguments to =sbatch=. 

  A batch script is a bash script with some special preprocessor directives, as
  seen in the example below.

  #+begin_src bash
  #!/bin/bash
  #SBATCH --gres=gpu:titanx:2
  #SBATCH --mem=12G
  #SBATCH --output=test-sbatch.log
  echo "I have these GPUs:" $CUDA_VISIBLE_DEVICES
  echo "On this machine" $(hostname)
  exit 0
  #+end_src

  We send this job to the scheduler with
  #+begin_src
  sbatch example-job.sbatch
  #+end_src

  This defines a job that will request two NVidia Titan X GPUs, 12 GB of memory
  and write stdout/stderr to the file =test-sbatch.log= in the current
  directory. Once the scheduler is able to allocate the necessary resources it
  will execute the job, writing the IDs of the allocated GPUs and the hostname
  of the allocated node to =test-sbatch.log=.

  We can use =sacct= to see the job history and =squeue= to see queued and
  running jobs.
    
   - [[./slurm-usage.org][Learn more]]
   - [[https://slurm.schedmd.com/pdfs/summary.pdf][Cheatsheet]]

* Storage
  There are a few file systems available on Terra. *None of these are backed
  up*. All, except =/scratch=, are raided for fault-tolerance.

  Due to the nature of home, reading and writing to it slows Terra cluster down
  for everyone. So, do most of your modelling work or intensive reading/writing
  on /scratch or /work . Home should only be for your code repos, configuration
  files, etc. Your model and data directories should always be on /scratch or
  /work.

  | Mount path   | Purpose                                                   | Size    | Speed                       | local node |
  |--------------+-----------------------------------------------------------+---------+-----------------------------+------------|
  | /data        | Shared datasets, models and archives. Read-only for users.        | 2.7 TiB | Fast reads & slow writes    | terra      |
  | /scratch     | "Unimportant" temporary files with many writes and reads. | 2 TiB   | Fastest                     | terra      |
  | /mnt/scratch | Links to /scratch for legacy reasons                      |         |                             |            |
  | /work        | More important temporary files                            | 3.4 TiB | Fastest reads & fast writes | torpaq     |
  | /home        | Code, configuration files, etc                            | 5.4T    | Slow                        | terra      |
  |--------------+-----------------------------------------------------------+---------+-----------------------------+------------|

* Useful places
  Users have access to a few read-only folders on Terra.
  These places are meant to store frequently used corpora, models and tools.

  | Path         | Purpose                                          |
  |--------------+--------------------------------------------------|
  | /data        | Datasets and data used by and created by LVL |
  | /models      | Pretrained models from LVL or other sources      |
  | /data/tools  | Shared tools and libraries                       |

  If you want to add your own or additional data, models or libraries contact the admins.

* Containers
  [[https://sylabs.io/singularity/][Singularity]] ([[https://sylabs.io/singularity/faq/][FAQ]]) is a container solution for scientific computing that allows
  unprivileged use of containers. Singularity supports building its own images
  from scratch and ready-made Docker images.

  A user can build their own containerized application/project on there own
  machines which can be run on Terra in a Slurm batch job.

* Jupyter Notebooks (JupyterHub)
  Jupyter notebooks have become a popular way of doing scientific computing and
  interactive machine learning.

  LVL runs a JupyterHub accessible at https://terra.hir.is (RU intranet, you'll
  have to accept the self-signed cert) which allows users to spin up notebook
  servers through Slurm.

  The notebook server runs in a container using an image with a Python 3.7 Conda
  base environment. The /Conda/ tab allows you to create new environments, and
  new packages can be added to enviroments through the UI or in a notebook using
  a specific environment.

* Installing software
  An easy way for a user to install necessary tools and libraries, other than
  compiling things yourself, is to use the Conda package manager.

  To use it you first have to add it to your environment:

  #+begin_src shell
  source /data/tools/anaconda/etc/profile.d/conda.sh
  #+end_src

  Then, to always have conda available you can add it to your bash profile with:

  #+begin_src shell
  conda init
  #+end_src

  Let's say that for some reason you need to use =pdftotext= from
  =poppler-utils=, then you can create and environment specifically for that:
  #+begin_src shell
  conda create -n pdf-stuff poppler-utils
  #+end_src

  This will create an environment named =pdf-stuff= with the package
  =poppler-utils= and all of its dependencies installed. To activate it you run:
  #+begin_src shell
  conda activate pdf-stuff
  #+end_src

  To verify that it has been loaded:
  #+begin_src shell :eval never-export :exports both
  whereis pdftotext
  #+end_src

  #+RESULTS:
  #+begin_example
  pdftotext: /home/staff/rkjaran/.conda/envs/test-poppler-env/bin/pdftotext
  #+end_example
  
